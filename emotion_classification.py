# -*- coding: utf-8 -*-
"""Emotion_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LUEB1ZM4PryZp2rbpFesujr-RI4Flo9n
"""

#%tensorflow_version 1.x
from google.colab import drive
drive.mount('/content/drive')

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import string


def load_doc(filename):
	file = open(filename, 'r')
	text = file.read()
	file.close()
	return text

def clean_doc(doc):
	tokens = doc.split()
	table = str.maketrans('', '', string.punctuation)
	tokens = [w.translate(table) for w in tokens]
	tokens = [word for word in tokens if word.isalpha()]
	stop_words = set(stopwords.words('english'))
	tokens = [w for w in tokens if not w in stop_words]
	tokens = [word for word in tokens if len(word) > 1]
	return tokens

#filename = '/content/drive/My Drive/enotion_dataset_new11/angry/t0000.txt'
#text = load_doc(filename)
#tokens = clean_doc(text)
#print(tokens)

from string import punctuation
from os import listdir
from collections import Counter

def add_doc_to_vocab(filename, vocab):
	doc = load_doc(filename)
	tokens = clean_doc(doc)
	vocab.update(tokens)
 
def process_docs(directory, vocab, is_trian):
	for filename in listdir(directory):
		if is_trian and filename.startswith('v'):
			continue
		if not is_trian and not filename.startswith('v'):
			continue
		path = directory + '/' + filename
		add_doc_to_vocab(path, vocab)
 
vocab = Counter()
process_docs('/content/drive/My Drive/enotion_dataset_new11/angry', vocab, True)
print("done")
process_docs('/content/drive/My Drive/enotion_dataset_new11/happy', vocab, True)
print("done")
process_docs('/content/drive/My Drive/enotion_dataset_new11/neutral', vocab, True)
print("done")
process_docs('/content/drive/My Drive/enotion_dataset_new11/sad', vocab, True)
print("done")

print(len(vocab))
print(vocab.most_common(50))

min_occurane = 2
tokens = [k for k,c in vocab.items() if c >= min_occurane]
print(len(tokens))

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd

comment_words = ""
stopwords = set(STOPWORDS)
comment_words += " ".join(tokens)+" "
#print(comment_words)
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='black',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)
  
# plot the WordCloud image                       
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
  
plt.show()

def save_list(lines, filename):
	data = '\n'.join(lines)
	file = open(filename, 'w')
	file.write(data)
	file.close()
 
save_list(tokens, 'vocab.txt')

from string import punctuation
from os import listdir
from numpy import array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential, model_from_json
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

def load_doc(filename):
	file = open(filename, 'r')
	text = file.read()
	file.close()
	return text

def clean_doc(doc, vocab):
	tokens = doc.split()
	table = str.maketrans('', '', punctuation)
	tokens = [w.translate(table) for w in tokens]
	tokens = [w for w in tokens if w in vocab]
	tokens = ' '.join(tokens)
	return tokens

def process_docs(directory, vocab, is_trian):
	documents = list()
	for filename in listdir(directory):
		if is_trian and filename.startswith('v'):
			continue
		if not is_trian and not filename.startswith('v'):
			continue
		path = directory + '/' + filename
		doc = load_doc(path)
		tokens = clean_doc(doc, vocab)
		documents.append(tokens)
	return documents

vocab_filename = 'vocab.txt'
vocab = load_doc(vocab_filename)
vocab = vocab.split()
vocab = set(vocab)

angry_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/angry', vocab, True)
happy_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/happy', vocab, True)
neutral_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/neutral', vocab, True)
sad_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/sad', vocab, True)
train_docs = angry_docs + happy_docs + neutral_docs + sad_docs

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_docs)

encoded_docs = tokenizer.texts_to_sequences(train_docs)
max_length = max([len(s.split()) for s in train_docs])
Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
ytrain = array([0 for _ in range(2495)] + [1 for _ in range(2688)] + [2 for _ in range(2693)] + [3 for _ in range(2480)])

angry_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/angry', vocab, False)
happy_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/happy', vocab, False)
neutral_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/neutral', vocab, False)
sad_docs = process_docs('/content/drive/My Drive/enotion_dataset_new11/sad', vocab, False)
test_docs = angry_docs + happy_docs + neutral_docs + sad_docs

encoded_docs = tokenizer.texts_to_sequences(test_docs)

Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')

ytest = array([0 for _ in range(831)] + [1 for _ in range(896)] + [2 for _ in range(897)] + [3 for _ in range(828)])

vocab_size = len(tokenizer.word_index) + 1

print(ytrain)

print(max_length)
print(vocab_size)

model = Sequential()
model.add(Embedding(vocab_size, 100, input_length=max_length))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(4, activation='softmax'))
print(model.summary())

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(Xtrain, ytrain, epochs=10, verbose=2)

loss, acc = model.evaluate(Xtest, ytest, verbose=0)
print('Test Accuracy: %f' % (acc*100))

classes = ["angry","happy","neutral","sad"]

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import itertools

y_pred = model.predict(Xtest)
y_pred_class = np.argmax(y_pred,axis=1)

cnf_matrix = confusion_matrix(ytest, y_pred_class)
def plot_confusion_matrix(cm, labels, normalize = True, cmap=plt.cm.Blues):

  if normalize:
      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

  plt.imshow(cm, cmap=plt.cm.Blues)
  plt.colorbar()
  tick_marks = np.arange(len(labels))
  plt.xticks(tick_marks, labels, rotation=45)
  plt.yticks(tick_marks, labels)
  plt.ylabel('True label')
  plt.xlabel('Predicted label')

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
     plt.text(j, i, format(cm[i, j], fmt),
        horizontalalignment="center",
        color="white" if cm[i, j] > thresh else "black")

plt.figure(figsize=(10,10))
plot_confusion_matrix(cnf_matrix, labels=classes)

#print(cnf_matrix)

print(classification_report(ytest, y_pred_class, target_names=classes))

#text = ["I am not happy with this movie"]
text = ['this looks stupid.', "I am unhappy with this movie", 'disgusting movie', 'fucking bad movie', 'worst of all time', 'very emoional movie, i got tears']

from keras.preprocessing import sequence
sequences_test = tokenizer.texts_to_sequences(text)
#data_int_t = pad_sequences(sequences_test, padding='pre', maxlen=(max_length-5))
data_test = pad_sequences(sequences_test, padding='post', maxlen=(max_length))
y_prob = model.predict(data_test)
for n, prediction in enumerate(y_prob):
    pred = y_prob.argmax(axis=-1)[n]
    print(text[n],"\nPrediction:",classes[pred],"\n")

!pip install gTTS
from gtts import gTTS 
from IPython.display import Audio 
 
predtext = 'The emotion of sentence is ' + classes[pred]
language = 'en'

tts = gTTS(text=predtext, lang=language)
tts.save("emotion.mp3") 

Audio("emotion.mp3", autoplay=True)

